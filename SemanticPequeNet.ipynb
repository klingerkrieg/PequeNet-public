{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from Trainer import Trainer, compute_segmentation_metrics, compute_iou\n",
    "import numpy as np\n",
    "\n",
    "from util import count_trainable_parameters\n",
    "#jupyter nbconvert --to script SemanticPequeNet.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22868c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "178.993\n"
     ]
    }
   ],
   "source": [
    "class SemanticPequeNet(nn.Module):\n",
    "\n",
    "    width_modifier = 1.0\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                       width_modifier=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.width_modifier = width_modifier\n",
    "        width   = int(64 * width_modifier)\n",
    "        \n",
    "        self.pool       = nn.MaxPool2d(kernel_size=2)\n",
    "        self.upsample2  = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Features\n",
    "        self.fblock1 = self.features_block(in_channels,         width)\n",
    "        self.fblock2 = self.features_block(width+in_channels,   width)\n",
    "        self.fblock3 = self.features_block(width+in_channels,   width)\n",
    "        self.fblock4 = self.features_block(width+in_channels,   width)\n",
    "\n",
    "        # Group Conv Blocks\n",
    "        self.gconv4 = self.grouped_conv_block(width,       width)\n",
    "        self.gconv3 = self.grouped_conv_block(width,       width)\n",
    "        self.gconv2 = self.grouped_conv_block(width,       width)\n",
    "        self.gconv1 = self.grouped_conv_block(width,       width)\n",
    "        \n",
    "\n",
    "        # Final Layer\n",
    "        self.final    = nn.Conv2d(width, out_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def features_block(self, in_channels, out_channels):\n",
    "        kernel_size=3\n",
    "        return nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, groups=in_channels),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, groups=in_channels),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    \n",
    "    def grouped_conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels//2),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, groups=in_channels//2),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #print(\"feat1\")\n",
    "        lo1 = self.fblock1(x)\n",
    "        x1  = self.pool(x) #128\n",
    "\n",
    "        #print(\"feat2\")\n",
    "        lx1 = torch.cat((self.pool(lo1), x1), dim=1)\n",
    "        lo2 = self.fblock2(lx1)\n",
    "        x2 = self.pool(x1) #64\n",
    "        \n",
    "        #print(\"feat3\")\n",
    "        lx2 = torch.cat((self.pool(lo2), x2), dim=1)\n",
    "        lo3 = self.fblock3(lx2)\n",
    "        x3 = self.pool(x2) #32\n",
    "\n",
    "        #print(\"feat4\")\n",
    "        lx3 = torch.cat((self.pool(lo3), x3), dim=1)\n",
    "        lo4 = self.fblock4(lx3)\n",
    "\n",
    "        ## Group Conv Blocks\n",
    "        #print(\"conv4\")\n",
    "        out4  = self.gconv4(lo4)\n",
    "        \n",
    "        #print(\"conv3\")\n",
    "        out3  = self.upsample2(out4) #64\n",
    "        out3 = torch.sum(torch.stack((out3, lo3), dim=1), dim=1)\n",
    "        out3  = self.gconv3(out3)\n",
    "\n",
    "        #print(\"conv2\")\n",
    "        out2  = self.upsample2(out3) #128\n",
    "        out2 = torch.sum(torch.stack((out2, lo2), dim=1), dim=1)\n",
    "        out2  = self.gconv2(out2)\n",
    "\n",
    "        #print(\"conv1\")\n",
    "        out1 = self.upsample2(out2) #256\n",
    "        out1 = torch.sum(torch.stack((out1, lo1), dim=1), dim=1)\n",
    "        out1 = self.gconv1(out1)\n",
    "    \n",
    "        \n",
    "        return self.final(out1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = SemanticPequeNet(in_channels=3, out_channels=1)\n",
    "    x = torch.randn(1, 3, 256, 256)\n",
    "    y = model(x)\n",
    "    print(y.shape)\n",
    "    print(count_trainable_parameters(model, format=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_SemanticPequeNet_model(model_file_dir, in_channels=3, out_channels=1, width_modifier=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SemanticPequeNet(in_channels=in_channels, out_channels=out_channels, width_modifier=width_modifier)\n",
    "    checkpoint = torch.load(model_file_dir, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec99a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SemanticTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, num_classes, model_filename=None, model_dir=None, info={}, save_xlsx=False):\n",
    "        super(SemanticTrainer, self).__init__(model_filename=model_filename, model_dir=model_dir, info=info, save_xlsx=save_xlsx)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def create_criterion(self):\n",
    "        self.info['loss_function'] = 'CrossEntropyLoss'\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    \n",
    "    def train_loop(self, images, masks, epoch):\n",
    "        outputs     = self.get_model_output(images)\n",
    "\n",
    "        masks_s     = masks.long().squeeze(1)\n",
    "\n",
    "        loss    = self.criterion(outputs, masks_s)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        train_loss = loss.item() * images.size(0)\n",
    "\n",
    "        return train_loss\n",
    "\n",
    "\n",
    "    def val_loop(self, images, masks):\n",
    "        outputs     = self.get_model_output(images)\n",
    "\n",
    "        masks_s     = masks.long().squeeze(1)\n",
    "\n",
    "        loss        = self.criterion(outputs, masks_s)\n",
    "        val_loss    = loss.item() * images.size(0)\n",
    "        \n",
    "        preds       = torch.argmax(outputs, dim=1)\n",
    "        dice, mIoU, precision, recall, q = compute_segmentation_metrics(preds, masks, self.num_classes)\n",
    "        IoU = compute_iou(preds, masks, num_classes=self.num_classes)\n",
    "\n",
    "        val_dice      = dice      * images.size(0)\n",
    "        val_mIoU      = mIoU      * images.size(0)\n",
    "        val_IoU       = IoU       * images.size(0)\n",
    "        val_precision = precision * images.size(0)\n",
    "        val_recall    = recall    * images.size(0)\n",
    "        val_q         = q         * images.size(0)\n",
    "\n",
    "        return val_loss, val_dice, val_mIoU, val_IoU, val_precision, val_recall, val_q\n",
    "    \n",
    "    def _save_output(self, epoch=None):\n",
    "        with torch.no_grad():\n",
    "            output     = self.get_model_output(self.sample_img_fixed) # [1, C, H, W]\n",
    "            pred_mask = torch.argmax(output[0], dim=0).cpu().numpy()  # [H, W]\n",
    "            gt_mask = self.sample_mask_fixed.squeeze().cpu().numpy()  # [H, W]\n",
    "\n",
    "            h, w = gt_mask.shape\n",
    "            diff_img = np.zeros((h, w, 3), dtype=np.float32)\n",
    "\n",
    "            # Diferença entre máscara predita e real\n",
    "            tp = pred_mask == gt_mask                    # Acerto (classe correta)\n",
    "            fp = (pred_mask != gt_mask) & (gt_mask == 0) # Erro onde o ground truth era fundo\n",
    "            fn = (pred_mask != gt_mask) & (gt_mask != 0) # Erro onde o ground truth era classe\n",
    "\n",
    "            # Visualização\n",
    "            diff_img[tp] = [0, 1, 0]       # Branco para TP\n",
    "            diff_img[fn] = [1, 0.5, 0]     # Laranja para FN\n",
    "            diff_img[fp] = [1, 0, 0]       # Vermelho para FP\n",
    "\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            axs[0].imshow(gt_mask, cmap='viridis', vmin=0, vmax=self.num_classes - 1)\n",
    "            axs[0].set_title(\"Ground Truth\")\n",
    "            axs[0].axis('off')\n",
    "\n",
    "            axs[1].imshow(pred_mask, cmap='viridis', vmin=0, vmax=self.num_classes - 1)\n",
    "            axs[1].set_title(\"Predição\")\n",
    "            axs[1].axis('off')\n",
    "\n",
    "            axs[2].imshow(diff_img)\n",
    "            axs[2].set_title(\"Diferença (TP=branco, FN=laranja, FP=vermelho)\")\n",
    "            axs[2].axis('off')\n",
    "\n",
    "            suptitle = self.model_filename.replace('.pth', '')\n",
    "            if epoch is not None:\n",
    "                suptitle += f\" epoch:{epoch}\"\n",
    "\n",
    "            fig.suptitle(suptitle, fontsize=14)\n",
    "            return fig, axs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
